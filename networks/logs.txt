Cloning into '_master'...
remote: Enumerating objects: 1779, done.
remote: Counting objects: 100% (1779/1779), done.
remote: Compressing objects: 100% (1536/1536), done.
remote: Total 1779 (delta 334), reused 1656 (delta 211), pack-reused 0
Receiving objects: 100% (1779/1779), 291.35 MiB | 33.28 MiB/s, done.
Resolving deltas: 100% (334/334), done.
Checking out files: 100% (1250/1250), done.
>>> main module loaded ...
>>> handler module loadded ...
>>> class_BasicModel.py loadded ...
>>> configure.py loadded ...
>>> results.py loadded ...
>>> graphs.py loadded ...
>>> dataset.py loadded ...
>>> class_vgg16Seq.py loadded ...
>>> class_cnnFunctional.py loadded ...
>>> class_lstmConv2d.py loadded ...
>>> class_vgg16.py loadded ...
>>> class_cnnSeq.py loadded ...
>>> class_res.py loadded ...
>>> class_lstmBi.py loadded ...
>>> class_lstm.py loadded ...
>>> class_vgg_lstm.py loadded ...
>>> all modules loaded ...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> Tenserflow version: 2.7.0 - with /device:GPU:0
>>> Keras version: 2.7.0
Mounted at /content/drive
>>> List of all local devices:
['/device:CPU:0', '/device:GPU:0']
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> tensor configuration ...
>>> cannot configure tensorflow
>>> intial configurations done...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> reading TEST dataset ...
>>> TEST dimensions: (40, 512, 512, 3) , (40,)
========================================
>>> reading TRAIN dataset ...
>>> TRAIN dimensions: (40, 512, 512, 3) , (40,)
========================================
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: CnnFunctional with : program_0
>>> CnnFunctional model intiated ...
>>> bulding CnnFunctional model ...
>>> showing CnnFunctional summery ...
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 512, 512, 3)]     0         
                                                                 
 flatten (Flatten)           (None, 786432)            0         
                                                                 
 dense (Dense)               (None, 512)               402653696 
                                                                 
 dropout (Dropout)           (None, 512)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                5130      
                                                                 
=================================================================
Total params: 402,658,826
Trainable params: 402,658,826
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: CnnFunctional
>>> training CnnFunctional model ...
8/8 [==============================] - 4s 163ms/step - loss: 2457.9797 - accuracy: 0.3250
>>> testing CnnFunctional model ...
2/2 - 0s - loss: 339.9829 - accuracy: 0.5000 - 259ms/epoch - 129ms/step
>>> Restored model, accuracy: 50.00%
>>> successful model: CnnFunctional with : program_0
xxx deleted, model CnnFunctional
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: CnnSeq with : program_0
>>> CnnSeq model intiated ...
>>> bulding CnnSeq model ...
XXX Error in model: CnnSeq with : program_0 failed to allocate memory [Op:AddV2]
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: res with : program_0
>>> res model intiated ...
xxx deleted, model CnnSeq
>>> bulding res model ...
>>> showing res summery ...
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 512, 512, 3)]     0         
                                                                 
 batch_normalization (BatchN  (None, 512, 512, 3)      12        
 ormalization)                                                   
                                                                 
 conv2d (Conv2D)             (None, 512, 512, 32)      896       
                                                                 
 re_lu (ReLU)                (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_1 (Batc  (None, 512, 512, 32)     128       
 hNormalization)                                                 
                                                                 
 conv2d_1 (Conv2D)           (None, 512, 512, 32)      9248      
                                                                 
 re_lu_1 (ReLU)              (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_2 (Batc  (None, 512, 512, 32)     128       
 hNormalization)                                                 
                                                                 
 conv2d_2 (Conv2D)           (None, 512, 512, 32)      9248      
                                                                 
 re_lu_2 (ReLU)              (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_3 (Batc  (None, 512, 512, 32)     128       
 hNormalization)                                                 
                                                                 
 conv2d_3 (Conv2D)           (None, 512, 512, 32)      9248      
                                                                 
 re_lu_3 (ReLU)              (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_4 (Batc  (None, 512, 512, 32)     128       
 hNormalization)                                                 
                                                                 
 conv2d_4 (Conv2D)           (None, 512, 512, 32)      9248      
                                                                 
 re_lu_4 (ReLU)              (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_5 (Batc  (None, 512, 512, 32)     128       
 hNormalization)                                                 
                                                                 
 conv2d_5 (Conv2D)           (None, 256, 256, 64)      18496     
                                                                 
 re_lu_5 (ReLU)              (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_6 (Batc  (None, 256, 256, 64)     256       
 hNormalization)                                                 
                                                                 
 conv2d_6 (Conv2D)           (None, 256, 256, 64)      36928     
                                                                 
 re_lu_6 (ReLU)              (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_7 (Batc  (None, 256, 256, 64)     256       
 hNormalization)                                                 
                                                                 
 conv2d_7 (Conv2D)           (None, 256, 256, 64)      36928     
                                                                 
 re_lu_7 (ReLU)              (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_8 (Batc  (None, 256, 256, 64)     256       
 hNormalization)                                                 
                                                                 
 conv2d_8 (Conv2D)           (None, 256, 256, 64)      36928     
                                                                 
 re_lu_8 (ReLU)              (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_9 (Batc  (None, 256, 256, 64)     256       
 hNormalization)                                                 
                                                                 
 average_pooling2d (AverageP  (None, 64, 64, 64)       0         
 ooling2D)                                                       
                                                                 
 flatten_2 (Flatten)         (None, 262144)            0         
                                                                 
 dense_3 (Dense)             (None, 10)                2621450   
                                                                 
=================================================================
Total params: 2,790,294
Trainable params: 2,789,456
Non-trainable params: 838
_________________________________________________________________
>>> plotting model: res
>>> training res model ...
8/8 [==============================] - 37s 668ms/step - loss: 29.0177 - accuracy: 0.7750
>>> testing res model ...
2/2 - 9s - loss: 0.3989 - accuracy: 0.7500 - 9s/epoch - 4s/step
>>> Restored model, accuracy: 75.00%
>>> successful model: res with : program_0
xxx deleted, model res
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: vgg16 with : program_0
>>> vgg16 model intiated ...
>>> bulding vgg16 model ...
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
58892288/58889256 [==============================] - 1s 0us/step
58900480/58889256 [==============================] - 1s 0us/step
>>> showing vgg16 summery ...
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 512, 512, 3)]     0         
                                                                 
 block1_conv1 (Conv2D)       (None, 512, 512, 64)      1792      
                                                                 
 block1_conv2 (Conv2D)       (None, 512, 512, 64)      36928     
                                                                 
 block1_pool (MaxPooling2D)  (None, 256, 256, 64)      0         
                                                                 
 block2_conv1 (Conv2D)       (None, 256, 256, 128)     73856     
                                                                 
 block2_conv2 (Conv2D)       (None, 256, 256, 128)     147584    
                                                                 
 block2_pool (MaxPooling2D)  (None, 128, 128, 128)     0         
                                                                 
 block3_conv1 (Conv2D)       (None, 128, 128, 256)     295168    
                                                                 
 block3_conv2 (Conv2D)       (None, 128, 128, 256)     590080    
                                                                 
 block3_conv3 (Conv2D)       (None, 128, 128, 256)     590080    
                                                                 
 block3_pool (MaxPooling2D)  (None, 64, 64, 256)       0         
                                                                 
 block4_conv1 (Conv2D)       (None, 64, 64, 512)       1180160   
                                                                 
 block4_conv2 (Conv2D)       (None, 64, 64, 512)       2359808   
                                                                 
 block4_conv3 (Conv2D)       (None, 64, 64, 512)       2359808   
                                                                 
 block4_pool (MaxPooling2D)  (None, 32, 32, 512)       0         
                                                                 
 block5_conv1 (Conv2D)       (None, 32, 32, 512)       2359808   
                                                                 
 block5_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   
                                                                 
 block5_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   
                                                                 
 block5_pool (MaxPooling2D)  (None, 16, 16, 512)       0         
                                                                 
 global_average_pooling2d (G  (None, 512)              0         
 lobalAveragePooling2D)                                          
                                                                 
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: vgg16
>>> training vgg16 model ...
8/8 [==============================] - 22s 1s/step - loss: 12.1129 - accuracy: 0.2250
>>> testing vgg16 model ...
2/2 - 42s - loss: 10.2563 - accuracy: 0.5000 - 42s/epoch - 21s/step
>>> Restored model, accuracy: 50.00%
>>> successful model: vgg16 with : program_0
xxx deleted, model vgg16
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: vgg16seq with : program_0
XXX Error in model: vgg16seq with : program_0 'vgg16seq'
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: vgg16Seq with : program_0
>>> vgg16Seq model intiated ...
>>> bulding vgg16Seq model ...
/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(SGD, self).__init__(name, **kwargs)
>>> showing vgg16Seq summery ...
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_9 (Conv2D)           (None, 512, 512, 64)      1792      
                                                                 
 conv2d_10 (Conv2D)          (None, 512, 512, 64)      36928     
                                                                 
 max_pooling2d (MaxPooling2D  (None, 256, 256, 64)     0         
 )                                                               
                                                                 
 conv2d_11 (Conv2D)          (None, 256, 256, 128)     73856     
                                                                 
 conv2d_12 (Conv2D)          (None, 256, 256, 128)     147584    
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 128, 128, 128)    0         
 2D)                                                             
                                                                 
 conv2d_13 (Conv2D)          (None, 128, 128, 256)     295168    
                                                                 
 conv2d_14 (Conv2D)          (None, 128, 128, 256)     590080    
                                                                 
 conv2d_15 (Conv2D)          (None, 128, 128, 256)     590080    
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 64, 64, 256)      0         
 2D)                                                             
                                                                 
 conv2d_16 (Conv2D)          (None, 64, 64, 512)       1180160   
                                                                 
 conv2d_17 (Conv2D)          (None, 64, 64, 512)       2359808   
                                                                 
 conv2d_18 (Conv2D)          (None, 64, 64, 512)       2359808   
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 32, 32, 512)      0         
 2D)                                                             
                                                                 
 conv2d_19 (Conv2D)          (None, 32, 32, 512)       2359808   
                                                                 
 conv2d_20 (Conv2D)          (None, 32, 32, 512)       2359808   
                                                                 
 conv2d_21 (Conv2D)          (None, 32, 32, 512)       2359808   
                                                                 
 vgg16 (MaxPooling2D)        (None, 16, 16, 512)       0         
                                                                 
 flatten (Flatten)           (None, 131072)            0         
                                                                 
 fc1 (Dense)                 (None, 256)               33554688  
                                                                 
 fc2 (Dense)                 (None, 128)               32896     
                                                                 
 output (Dense)              (None, 1)                 129       
                                                                 
=================================================================
Total params: 48,302,401
Trainable params: 48,302,401
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: vgg16Seq
>>> training vgg16Seq model ...
8/8 [==============================] - 10s 1s/step - loss: 0.6935 - accuracy: 0.5000
>>> testing vgg16Seq model ...
2/2 - 2s - loss: 0.6937 - accuracy: 0.5000 - 2s/epoch - 1s/step
>>> Restored model, accuracy: 50.00%
>>> successful model: vgg16Seq with : program_0
xxx deleted, model vgg16Seq
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: LstmConv2d with : program_0
>>> LstmConv2d model intiated ...
>>> bulding LstmConv2d model ...
(None, 2, 59, 59, 320)
tf.Tensor(
[[[[[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   ...

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]]


  [[[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   ...

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]]]



 [[[[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   ...

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]]


  [[[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   ...

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]]]



 [[[[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   ...

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]]


  [[[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   ...

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]]]



 ...



 [[[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]]



 [[[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]]



 [[[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]]], shape=(20, 2, 512, 512, 3), dtype=float32)
(20, 2, 512, 512, 3)
>>> showing LstmConv2d summery ...
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv_lstm2d (ConvLSTM2D)    (None, 2, 506, 506, 32)   219648    
                                                                 
 activation (Activation)     (None, 2, 506, 506, 32)   0         
                                                                 
 max_pooling3d (MaxPooling3D  (None, 2, 253, 253, 32)  0         
 )                                                               
                                                                 
 conv_lstm2d_1 (ConvLSTM2D)  (None, 2, 249, 249, 64)   614656    
                                                                 
 max_pooling3d_1 (MaxPooling  (None, 2, 124, 124, 64)  0         
 3D)                                                             
                                                                 
 conv_lstm2d_2 (ConvLSTM2D)  (None, 2, 122, 122, 96)   553344    
                                                                 
 activation_1 (Activation)   (None, 2, 122, 122, 96)   0         
                                                                 
 conv_lstm2d_3 (ConvLSTM2D)  (None, 2, 120, 120, 96)   663936    
                                                                 
 activation_2 (Activation)   (None, 2, 120, 120, 96)   0         
                                                                 
 conv_lstm2d_4 (ConvLSTM2D)  (None, 2, 118, 118, 96)   663936    
                                                                 
 max_pooling3d_2 (MaxPooling  (None, 2, 59, 59, 96)    0         
 3D)                                                             
                                                                 
 dense_4 (Dense)             (None, 2, 59, 59, 320)    31040     
                                                                 
 activation_3 (Activation)   (None, 2, 59, 59, 320)    0         
                                                                 
 dropout_1 (Dropout)         (None, 2, 59, 59, 320)    0         
                                                                 
 reshape (Reshape)           (None, 2, 1113920)        0         
                                                                 
 lstm (LSTM)                 (None, 2)                 8911384   
                                                                 
 dropout_2 (Dropout)         (None, 2)                 0         
                                                                 
 dense_5 (Dense)             (None, 2)                 6         
                                                                 
=================================================================
Total params: 11,657,950
Trainable params: 11,657,950
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: LstmConv2d
>>> training LstmConv2d model ...
4/4 [==============================] - 52s 7s/step - loss: 0.6931 - accuracy: 0.6000
>>> testing LstmConv2d model ...
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05c136b830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
1/1 - 21s - loss: 0.6931 - accuracy: 1.0000 - 21s/epoch - 21s/step
>>> Restored model, accuracy: 100.00%
>>> successful model: LstmConv2d with : program_0
xxx deleted, model LstmConv2d
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: lstm with : program_0
>>> lstm model intiated ...
>>> bulding lstm model ...
tf.Tensor(
[[ 11.   52.   66.   88.   91. ]
 [100.    1.1  11.   52.   66. ]], shape=(2, 5), dtype=float32)
(2, 5)
>>> showing lstm summery ...
Model: "sequential_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_1 (LSTM)               (None, 5, 1)              12        
                                                                 
 dense_6 (Dense)             (None, 5, 1)              2         
                                                                 
 activation_4 (Activation)   (None, 5, 1)              0         
                                                                 
=================================================================
Total params: 14
Trainable params: 14
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: lstm
>>> training lstm model ...
1/1 [==============================] - 3s 3s/step - loss: 11.2661 - accuracy: 0.6000
>>> testing lstm model ...
WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05c1aef170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
1/1 - 1s - loss: 11.2661 - accuracy: 0.6000 - 595ms/epoch - 595ms/step
>>> Restored model, accuracy: 60.00%
>>> successful model: lstm with : program_0
xxx deleted, model lstm
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: LstmBi with : program_0
>>> LstmBi model intiated ...
>>> bulding LstmBi model ...
tf.Tensor(
[[ 11.   52.   66.   88.   91. ]
 [100.    1.1  11.   52.   66. ]], shape=(2, 5), dtype=float32)
(2, 5)
>>> showing LstmBi summery ...
Model: "sequential_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 bidirectional (Bidirectiona  (None, 5, 2)             24        
 l)                                                              
                                                                 
 bidirectional_1 (Bidirectio  (None, 5, 2)             32        
 nal)                                                            
                                                                 
 dense_7 (Dense)             (None, 5, 1)              3         
                                                                 
 activation_5 (Activation)   (None, 5, 1)              0         
                                                                 
=================================================================
Total params: 59
Trainable params: 59
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: LstmBi
>>> training LstmBi model ...
1/1 [==============================] - 8s 8s/step - loss: 11.2661 - accuracy: 0.6000
>>> testing LstmBi model ...
1/1 - 2s - loss: 11.2661 - accuracy: 0.6000 - 2s/epoch - 2s/step
>>> Restored model, accuracy: 60.00%
>>> successful model: LstmBi with : program_0
xxx deleted, model LstmBi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> final results: 
 CnnFunctional with : program_0 ==> successful 
CnnSeq with : program_0 ==> failed 
res with : program_0 ==> successful 
vgg16 with : program_0 ==> successful 
vgg16seq with : program_0 ==> failed 
vgg16Seq with : program_0 ==> successful 
LstmConv2d with : program_0 ==> successful 
lstm with : program_0 ==> successful 
LstmBi with : program_0 ==> successful 
--- execution time: 5 minutes , 24.845 seconds ---