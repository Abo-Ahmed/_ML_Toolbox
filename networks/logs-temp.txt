Cloning into '_master'...
remote: Enumerating objects: 1834, done.
remote: Counting objects: 100% (1834/1834), done.
remote: Compressing objects: 100% (1564/1564), done.
remote: Total 1834 (delta 372), reused 1700 (delta 238), pack-reused 0
Receiving objects: 100% (1834/1834), 291.36 MiB | 31.86 MiB/s, done.
Resolving deltas: 100% (372/372), done.
Checking out files: 100% (1251/1251), done.
>>> main module loaded ...
>>> handler module loadded ...
>>> class_BasicModel.py loadded ...
>>> configure.py loadded ...
>>> results.py loadded ...
>>> graphs.py loadded ...
>>> dataset.py loadded ...
>>> class_vgg16Seq.py loadded ...
>>> class_cnnFunctional.py loadded ...
>>> class_lstmConv2d.py loadded ...
>>> class_vgg16.py loadded ...
>>> class_cnnSeq.py loadded ...
>>> class_ResNet.py loadded ...
>>> class_VggLstm.py loadded ...
>>> class_lstmBi.py loadded ...
>>> class_lstm.py loadded ...
>>> all modules loaded ...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> Tenserflow version: 2.7.0 - with /device:GPU:0
>>> Keras version: 2.7.0
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
>>> List of all local devices:
['/device:CPU:0', '/device:GPU:0']
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> tensor configuration ...
>>> cannot configure tensorflow
>>> intial configurations done...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> reading TEST dataset ...
>>> TEST dimensions: (40, 512, 512, 3) , (40,)
========================================
>>> reading TRAIN dataset ...
>>> TRAIN dimensions: (40, 512, 512, 3) , (40,)
========================================
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: CnnSeq with : program_0
>>> CnnSeq model intiated ...
>>> bulding CnnSeq model ...
>>> showing CnnSeq summery ...
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_3 (Flatten)         (None, 786432)            0         
                                                                 
 dense_10 (Dense)            (None, 512)               402653696 
                                                                 
 dropout_3 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 10)                5130      
                                                                 
=================================================================
Total params: 402,658,826
Trainable params: 402,658,826
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: CnnSeq
>>> training CnnSeq model ...
WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0d1b63a200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
8/8 [==============================] - 2s 159ms/step - loss: 1996.2175 - accuracy: 0.3500
>>> testing CnnSeq model ...
2/2 - 0s - loss: 1856.8184 - accuracy: 0.5000 - 256ms/epoch - 128ms/step
>>> Restored model, accuracy: 50.00%
>>> successful model: CnnSeq with : program_0
xxx deleted, model CnnSeq
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: CnnFunctional with : program_0
>>> CnnFunctional model intiated ...
>>> bulding CnnFunctional model ...
XXX Error in model: CnnFunctional with : program_0 failed to allocate memory [Op:AddV2]
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: ResNet with : program_0
>>> ResNet model intiated ...
xxx deleted, model CnnFunctional
>>> bulding ResNet model ...
>>> showing ResNet summery ...
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 512, 512, 3)]     0         
                                                                 
 batch_normalization_10 (Bat  (None, 512, 512, 3)      12        
 chNormalization)                                                
                                                                 
 conv2d_22 (Conv2D)          (None, 512, 512, 32)      896       
                                                                 
 re_lu_9 (ReLU)              (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_11 (Bat  (None, 512, 512, 32)     128       
 chNormalization)                                                
                                                                 
 conv2d_23 (Conv2D)          (None, 512, 512, 32)      9248      
                                                                 
 re_lu_10 (ReLU)             (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_12 (Bat  (None, 512, 512, 32)     128       
 chNormalization)                                                
                                                                 
 conv2d_24 (Conv2D)          (None, 512, 512, 32)      9248      
                                                                 
 re_lu_11 (ReLU)             (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_13 (Bat  (None, 512, 512, 32)     128       
 chNormalization)                                                
                                                                 
 conv2d_25 (Conv2D)          (None, 512, 512, 32)      9248      
                                                                 
 re_lu_12 (ReLU)             (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_14 (Bat  (None, 512, 512, 32)     128       
 chNormalization)                                                
                                                                 
 conv2d_26 (Conv2D)          (None, 512, 512, 32)      9248      
                                                                 
 re_lu_13 (ReLU)             (None, 512, 512, 32)      0         
                                                                 
 batch_normalization_15 (Bat  (None, 512, 512, 32)     128       
 chNormalization)                                                
                                                                 
 conv2d_27 (Conv2D)          (None, 256, 256, 64)      18496     
                                                                 
 re_lu_14 (ReLU)             (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_16 (Bat  (None, 256, 256, 64)     256       
 chNormalization)                                                
                                                                 
 conv2d_28 (Conv2D)          (None, 256, 256, 64)      36928     
                                                                 
 re_lu_15 (ReLU)             (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_17 (Bat  (None, 256, 256, 64)     256       
 chNormalization)                                                
                                                                 
 conv2d_29 (Conv2D)          (None, 256, 256, 64)      36928     
                                                                 
 re_lu_16 (ReLU)             (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_18 (Bat  (None, 256, 256, 64)     256       
 chNormalization)                                                
                                                                 
 conv2d_30 (Conv2D)          (None, 256, 256, 64)      36928     
                                                                 
 re_lu_17 (ReLU)             (None, 256, 256, 64)      0         
                                                                 
 batch_normalization_19 (Bat  (None, 256, 256, 64)     256       
 chNormalization)                                                
                                                                 
 average_pooling2d_1 (Averag  (None, 64, 64, 64)       0         
 ePooling2D)                                                     
                                                                 
 flatten_5 (Flatten)         (None, 262144)            0         
                                                                 
 dense_13 (Dense)            (None, 10)                2621450   
                                                                 
=================================================================
Total params: 2,790,294
Trainable params: 2,789,456
Non-trainable params: 838
_________________________________________________________________
>>> plotting model: ResNet
>>> training ResNet model ...
8/8 [==============================] - 8s 729ms/step - loss: 41.9841 - accuracy: 0.5250
>>> testing ResNet model ...
2/2 - 2s - loss: 5.7384 - accuracy: 0.5000 - 2s/epoch - 911ms/step
>>> Restored model, accuracy: 50.00%
>>> successful model: ResNet with : program_0
xxx deleted, model ResNet
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: Vgg16 with : program_0
>>> Vgg16 model intiated ...
>>> bulding Vgg16 model ...
>>> showing Vgg16 summery ...
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 512, 512, 3)]     0         
                                                                 
 block1_conv1 (Conv2D)       (None, 512, 512, 64)      1792      
                                                                 
 block1_conv2 (Conv2D)       (None, 512, 512, 64)      36928     
                                                                 
 block1_pool (MaxPooling2D)  (None, 256, 256, 64)      0         
                                                                 
 block2_conv1 (Conv2D)       (None, 256, 256, 128)     73856     
                                                                 
 block2_conv2 (Conv2D)       (None, 256, 256, 128)     147584    
                                                                 
 block2_pool (MaxPooling2D)  (None, 128, 128, 128)     0         
                                                                 
 block3_conv1 (Conv2D)       (None, 128, 128, 256)     295168    
                                                                 
 block3_conv2 (Conv2D)       (None, 128, 128, 256)     590080    
                                                                 
 block3_conv3 (Conv2D)       (None, 128, 128, 256)     590080    
                                                                 
 block3_pool (MaxPooling2D)  (None, 64, 64, 256)       0         
                                                                 
 block4_conv1 (Conv2D)       (None, 64, 64, 512)       1180160   
                                                                 
 block4_conv2 (Conv2D)       (None, 64, 64, 512)       2359808   
                                                                 
 block4_conv3 (Conv2D)       (None, 64, 64, 512)       2359808   
                                                                 
 block4_pool (MaxPooling2D)  (None, 32, 32, 512)       0         
                                                                 
 block5_conv1 (Conv2D)       (None, 32, 32, 512)       2359808   
                                                                 
 block5_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   
                                                                 
 block5_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   
                                                                 
 block5_pool (MaxPooling2D)  (None, 16, 16, 512)       0         
                                                                 
 global_average_pooling2d_2   (None, 512)              0         
 (GlobalAveragePooling2D)                                        
                                                                 
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: Vgg16
>>> training Vgg16 model ...
8/8 [==============================] - 10s 1s/step - loss: 9.0338 - accuracy: 0.4500
>>> testing Vgg16 model ...
2/2 - 2s - loss: 8.0591 - accuracy: 0.5000 - 2s/epoch - 1s/step
>>> Restored model, accuracy: 50.00%
>>> successful model: Vgg16 with : program_0
xxx deleted, model Vgg16
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: vgg16Seq with : program_0
>>> vgg16Seq model intiated ...
>>> bulding vgg16Seq model ...
/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(SGD, self).__init__(name, **kwargs)
>>> showing vgg16Seq summery ...
Model: "sequential_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_31 (Conv2D)          (None, 512, 512, 64)      1792      
                                                                 
 conv2d_32 (Conv2D)          (None, 512, 512, 64)      36928     
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 256, 256, 64)     0         
 2D)                                                             
                                                                 
 conv2d_33 (Conv2D)          (None, 256, 256, 128)     73856     
                                                                 
 conv2d_34 (Conv2D)          (None, 256, 256, 128)     147584    
                                                                 
 max_pooling2d_5 (MaxPooling  (None, 128, 128, 128)    0         
 2D)                                                             
                                                                 
 conv2d_35 (Conv2D)          (None, 128, 128, 256)     295168    
                                                                 
 conv2d_36 (Conv2D)          (None, 128, 128, 256)     590080    
                                                                 
 conv2d_37 (Conv2D)          (None, 128, 128, 256)     590080    
                                                                 
 max_pooling2d_6 (MaxPooling  (None, 64, 64, 256)      0         
 2D)                                                             
                                                                 
 conv2d_38 (Conv2D)          (None, 64, 64, 512)       1180160   
                                                                 
 conv2d_39 (Conv2D)          (None, 64, 64, 512)       2359808   
                                                                 
 conv2d_40 (Conv2D)          (None, 64, 64, 512)       2359808   
                                                                 
 max_pooling2d_7 (MaxPooling  (None, 32, 32, 512)      0         
 2D)                                                             
                                                                 
 conv2d_41 (Conv2D)          (None, 32, 32, 512)       2359808   
                                                                 
 conv2d_42 (Conv2D)          (None, 32, 32, 512)       2359808   
                                                                 
 conv2d_43 (Conv2D)          (None, 32, 32, 512)       2359808   
                                                                 
 vgg16 (MaxPooling2D)        (None, 16, 16, 512)       0         
                                                                 
 flatten (Flatten)           (None, 131072)            0         
                                                                 
 fc1 (Dense)                 (None, 256)               33554688  
                                                                 
 fc2 (Dense)                 (None, 128)               32896     
                                                                 
 output (Dense)              (None, 1)                 129       
                                                                 
=================================================================
Total params: 48,302,401
Trainable params: 48,302,401
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: vgg16Seq
>>> training vgg16Seq model ...
8/8 [==============================] - 10s 1s/step - loss: 0.6914 - accuracy: 0.5000
>>> testing vgg16Seq model ...
2/2 - 2s - loss: 0.6903 - accuracy: 0.5000 - 2s/epoch - 1s/step
>>> Restored model, accuracy: 50.00%
>>> successful model: vgg16Seq with : program_0
xxx deleted, model vgg16Seq
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: VggLstm with : program_0
>>> VggLstm model intiated ...
>>> bulding VggLstm model ...
tf.Tensor(
[[[[[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   ...

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]

   [[2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    ...
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]
    [2.4196079  2.745098   0.01176471]]]


  [[[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   ...

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]

   [[1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    ...
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]
    [1.5686275  1.5686275  0.01176471]]]


  [[[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   ...

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]

   [[4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    ...
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]
    [4.7058825  3.137255   0.01176471]]]]



 [[[[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   ...

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]

   [[5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    ...
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]
    [5.2313724  3.9215686  0.01176471]]]


  [[[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   ...

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]

   [[2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    ...
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]
    [2.1764705  2.8235295  0.01176471]]]


  [[[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   ...

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]

   [[1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    ...
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]
    [1.5294118  2.1568627  0.01176471]]]]



 [[[[3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    ...
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]]

   [[3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    ...
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]]

   [[3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    ...
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]]

   ...

   [[3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    ...
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]]

   [[3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    ...
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]]

   [[3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    ...
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]
    [3.7058823  3.7058823  0.01176471]]]


  [[[3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    ...
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]]

   [[3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    ...
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]]

   [[3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    ...
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]]

   ...

   [[3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    ...
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]]

   [[3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    ...
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]]

   [[3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    ...
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]
    [3.137255   2.3529413  0.01176471]]]


  [[[2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    ...
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]]

   [[2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    ...
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]]

   [[2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    ...
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]]

   ...

   [[2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    ...
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]]

   [[2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    ...
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]]

   [[2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    ...
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]
    [2.3529413  3.137255   0.01176471]]]]



 ...



 [[[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]]



 [[[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]]



 [[[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]


  [[[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   ...

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]

   [[2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    ...
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]
    [2.0078433  2.0078433  0.01176471]]]]], shape=(13, 3, 512, 512, 3), dtype=float32)
(13, 3, 512, 512, 3)
>>> showing VggLstm summery ...
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 3, 512, 512, 3)]  0         
                                                                 
 time_distributed_1 (TimeDis  (None, 3, 512)           14714688  
 tributed)                                                       
                                                                 
 lstm_5 (LSTM)               (None, 3, 1)              2056      
                                                                 
 dense_14 (Dense)            (None, 3, 1)              2         
                                                                 
 dense_15 (Dense)            (None, 3, 1)              2         
                                                                 
=================================================================
Total params: 14,716,748
Trainable params: 14,716,748
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: VggLstm
>>> training VggLstm model ...
3/3 [==============================] - 9s 2s/step - loss: 1.6902 - accuracy: 0.5128
>>> testing VggLstm model ...
1/1 - 40s - loss: 1.6902 - accuracy: 0.5128 - 40s/epoch - 40s/step
>>> Restored model, accuracy: 51.28%
>>> successful model: VggLstm with : program_0
xxx deleted, model VggLstm
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: LstmConv2d with : program_0
>>> LstmConv2d model intiated ...
>>> bulding LstmConv2d model ...
(None, 2, 59, 59, 320)
>>> showing LstmConv2d summery ...
Model: "sequential_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv_lstm2d_5 (ConvLSTM2D)  (None, 2, 506, 506, 32)   219648    
                                                                 
 activation_6 (Activation)   (None, 2, 506, 506, 32)   0         
                                                                 
 max_pooling3d_3 (MaxPooling  (None, 2, 253, 253, 32)  0         
 3D)                                                             
                                                                 
 conv_lstm2d_6 (ConvLSTM2D)  (None, 2, 249, 249, 64)   614656    
                                                                 
 max_pooling3d_4 (MaxPooling  (None, 2, 124, 124, 64)  0         
 3D)                                                             
                                                                 
 conv_lstm2d_7 (ConvLSTM2D)  (None, 2, 122, 122, 96)   553344    
                                                                 
 activation_7 (Activation)   (None, 2, 122, 122, 96)   0         
                                                                 
 conv_lstm2d_8 (ConvLSTM2D)  (None, 2, 120, 120, 96)   663936    
                                                                 
 activation_8 (Activation)   (None, 2, 120, 120, 96)   0         
                                                                 
 conv_lstm2d_9 (ConvLSTM2D)  (None, 2, 118, 118, 96)   663936    
                                                                 
 max_pooling3d_5 (MaxPooling  (None, 2, 59, 59, 96)    0         
 3D)                                                             
                                                                 
 dense_16 (Dense)            (None, 2, 59, 59, 320)    31040     
                                                                 
 activation_9 (Activation)   (None, 2, 59, 59, 320)    0         
                                                                 
 dropout_4 (Dropout)         (None, 2, 59, 59, 320)    0         
                                                                 
 reshape_1 (Reshape)         (None, 2, 1113920)        0         
                                                                 
 lstm_6 (LSTM)               (None, 2)                 8911384   
                                                                 
 dropout_5 (Dropout)         (None, 2)                 0         
                                                                 
 dense_17 (Dense)            (None, 2)                 6         
                                                                 
=================================================================
Total params: 11,657,950
Trainable params: 11,657,950
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: LstmConv2d
>>> training LstmConv2d model ...
XXX Error in model: LstmConv2d with : program_0 in user code:

    File "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py", line 878, in train_function  *
        return step_function(self, iterator)
    File "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py", line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py", line 860, in run_step  **
        outputs = model.train_step(data)
    File "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py", line 808, in train_step
        y_pred = self(x, training=True)
    File "/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File "/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py", line 263, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer "{layer_name}" is '

    ValueError: Input 0 of layer "sequential_9" is incompatible with the layer: expected shape=(None, 2, 512, 512, 3), found shape=(None, 3, 512, 512, 3)

<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: Lstm with : program_0
>>> Lstm model intiated ...
xxx deleted, model LstmConv2d
>>> bulding Lstm model ...
tf.Tensor(
[[ 11.   52.   66.   88.   91. ]
 [100.    1.1  11.   52.   66. ]], shape=(2, 5), dtype=float32)
(2, 5)
>>> showing Lstm summery ...
Model: "sequential_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_7 (LSTM)               (None, 5, 1)              12        
                                                                 
 dense_18 (Dense)            (None, 5, 1)              2         
                                                                 
 activation_10 (Activation)  (None, 5, 1)              0         
                                                                 
=================================================================
Total params: 14
Trainable params: 14
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: Lstm
>>> training Lstm model ...
1/1 [==============================] - 2s 2s/step - loss: 11.2661 - accuracy: 0.6000
>>> testing Lstm model ...
1/1 - 1s - loss: 11.2661 - accuracy: 0.6000 - 626ms/epoch - 626ms/step
>>> Restored model, accuracy: 60.00%
>>> successful model: Lstm with : program_0
xxx deleted, model Lstm
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
>>> starting model: LstmBi with : program_0
>>> LstmBi model intiated ...
>>> bulding LstmBi model ...
tf.Tensor(
[[ 11.   52.   66.   88.   91. ]
 [100.    1.1  11.   52.   66. ]], shape=(2, 5), dtype=float32)
(2, 5)
>>> showing LstmBi summery ...
Model: "sequential_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 bidirectional_2 (Bidirectio  (None, 5, 2)             24        
 nal)                                                            
                                                                 
 bidirectional_3 (Bidirectio  (None, 5, 2)             32        
 nal)                                                            
                                                                 
 dense_19 (Dense)            (None, 5, 1)              3         
                                                                 
 activation_11 (Activation)  (None, 5, 1)              0         
                                                                 
=================================================================
Total params: 59
Trainable params: 59
Non-trainable params: 0
_________________________________________________________________
>>> plotting model: LstmBi
>>> training LstmBi model ...
1/1 [==============================] - 8s 8s/step - loss: 11.2661 - accuracy: 0.6000
>>> testing LstmBi model ...
1/1 - 2s - loss: 11.2661 - accuracy: 0.6000 - 2s/epoch - 2s/step
>>> Restored model, accuracy: 60.00%
>>> successful model: LstmBi with : program_0
xxx deleted, model LstmBi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>> final results: 
 CnnSeq with : program_0 ==> successful 
CnnFunctional with : program_0 ==> failed failed to allocate memory [Op:AddV2]
ResNet with : program_0 ==> successful 
Vgg16 with : program_0 ==> successful 
vgg16Seq with : program_0 ==> successful 
VggLstm with : program_0 ==> successful 
LstmConv2d with : program_0 ==> failed in user code:
Lstm with : program_0 ==> successful 
LstmBi with : program_0 ==> successful 
--- execution time: 2 minutes , 46.247 seconds ---